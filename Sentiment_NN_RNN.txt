import pandas as pd
import keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import numpy as np







df = pd.read_csv('C:/DL_coding/keras/chennai_reviews.csv', sep=',', header=0)
df = df[['Review_Text', 'Sentiment']].copy()
df.head()






df['Sentiment'] = pd.to_numeric(df.Sentiment, errors= 'coerce').dropna().astype(int)





df['Sentiment'].value_counts()





df['Sentiment'] = [1 if x > 2 else 0 for x in df.Sentiment]




df['Sentiment'].value_counts()




data, labels = (df['Review_Text'].astype(str).values, df['Sentiment'].values)



tokenizer = Tokenizer(lower= True)
tokenizer.fit_on_texts(data)

data_sequence = tokenizer.texts_to_sequences(data)
data_padded = pad_sequences(data_sequence, maxlen= 100, padding='post')


data_train, data_test, labels_train, labels_test = train_test_split(data_padded, labels, test_size= 0.15, random_state= 1)


batch_size = 64

data_train_split = data_train[2*batch_size:]
labels_train_split = labels_train[2*batch_size:]

data_validation_split = data_train[:2*batch_size]
labels_validation_split = labels_train[:2*batch_size]




vocab_size = len(tokenizer.word_counts.keys())+1
num_words = 100
embedding_len = 32

model = keras.models.Sequential([
                                 keras.layers.Embedding(vocab_size, embedding_len, input_length= num_words),
                                 keras.layers.GRU(64),
                                 keras.layers.Dense(1, activation= 'sigmoid')
])




model.compile(
    optimizer= 'sgd',
    loss= 'binary_crossentropy',
    metrics= ['accuracy']
)



model.fit(
    data_train_split,
    labels_train_split,
    batch_size= batch_size,
    epochs= 2,
    verbose= 1,
    validation_data= (data_validation_split, labels_validation_split),
   
)



scores = model.evaluate(data_test, labels_test, verbose= 0)
scores